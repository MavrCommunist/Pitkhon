Filaments play an important role in star formation, but the formation process of filaments themselves is still unclear. The high-mass star forming clump G286.21+0.17 (G286 for short) that contains an "L" type filament was thought to undergo global collapse. Our high resolution ALMA band 3 observations resolve the gas kinematics of G286 and reveal two sub-clumps with very different velocities inside it. We find that the "blue profile" (an indicator of gas infall) of HCO+ lines in single dish observations of G286 is actually caused by gas emission from the two sub-clumps rather than gas infall. We advise great caution in interpreting gas kinematics (e.g., infall) from line profiles toward distant massive clumps in single dish observations. Energetic outflows are identified in G286 but the outflows are not strong enough to drive expansion of the two sub-clumps. The two parts of the "L" type filament ("NW-SE" and "NE-SW" filaments) show prominent velocity gradients perpendicular to their major axes, indicating that they are likely formed due to large-scale compression flows. We argue that the large-scale compression flows could be induced by the expansion of nearby giant HII regions. The "NW-SE" and "NE-SW" filaments seem to be in collision, and a large amount of gas has been accumulated in the junction region where the most massive core G286c1 forms.
We present the first sub-mJy (≈0.7 mJy beam−1) survey to be completed below 100 MHz, which is over an order of magnitude deeper than previously achieved for widefield imaging of any field at these low frequencies. The high resolution (15×15 arcsec) image of the Boötes field at 34-75 MHz is made from 56 hours of observation with the LOw Frequency ARray (LOFAR) Low Band Antenna (LBA) system. The observations and data reduction, including direction-dependent calibration, are described here. We present a radio source catalogue containing 1,948 sources detected over an area of 23.6 deg2, with a peak flux density threshold of 5σ. Using existing datasets, we characterise the astrometric and flux density uncertainties, finding a positional uncertainty of ∼1.2 arcsec and a flux density scale uncertainty of about 5 per cent. Using the available deep 144-MHz data, we identified 144-MHz counterparts to all the 54-MHz sources, and produced a matched catalogue within the deep optical coverage area containing 829 sources. We calculate the Euclidean-normalised differential source counts and investigate the low-frequency radio source spectral indices between 54 and 144 MHz, both of which show a general flattening in the radio spectral indices for lower flux density sources, from ∼−0.75 at 144-MHz flux densities between 100-1000 mJy to ∼−0.5 at 144-MHz flux densities between 5-10 mJy, due to a growing population of star forming galaxies and compact core-dominated AGN.
We analyze the distribution of rest-frame U-V and V-J colors for star-forming galaxies at 0.5 < z < 2.5. Using stellar population synthesis, stochastic star formation histories, and a simple prescription for the dust attenuation that accounts for the shape and inclination of galaxies, we construct a model for the distribution of galaxy colors. With only two free parameters, this model is able to reproduce the observed galaxy colors as a function of redshift and stellar mass remarkably well. Our analysis suggests that the wide range of dust attenuation values measured for star-forming galaxies at a given redshift and stellar mass is almost entirely due to the effect of inclination; if all galaxies were observed edge-on, they would show very similar dust attenuation. This result has important implications for the interpretation of dust attenuation measurements, the treatment of UV and IR luminosity, and the comparison between numerical simulations and observations.
Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and Glow-TTS can synthesize high-quality speech from the given text in parallel. After analyzing two kinds of generative NAR-TTS models (VAE and normalizing flow), we find that: VAE is good at capturing the long-range semantics features (e.g., prosody) even with small model size but suffers from blurry and unnatural results; and normalizing flow is good at reconstructing the frequency bin-wise details but performs poorly when the number of model parameters is limited. Inspired by these observations, to generate diverse speech with natural details and rich prosody using a lightweight architecture, we propose PortaSpeech, a portable and high-quality generative text-to-speech model. Specifically, 1) to model both the prosody and mel-spectrogram details accurately, we adopt a lightweight VAE with an enhanced prior followed by a flow-based post-net with strong conditional inputs as the main architecture. 2) To further compress the model size and memory footprint, we introduce the grouped parameter sharing mechanism to the affine coupling layers in the post-net. 3) To improve the expressiveness of synthesized speech and reduce the dependency on accurate fine-grained alignment between text and speech, we propose a linguistic encoder with mixture alignment combining hard inter-word alignment and soft intra-word alignment, which explicitly extracts word-level semantic information. Experimental results show that PortaSpeech outperforms other TTS models in both voice quality and prosody modeling in terms of subjective and objective evaluation metrics, and shows only a slight performance degradation when reducing the model parameters to 6.7M (about 4x model size and 3x runtime memory compression ratio compared with FastSpeech 2). Our extensive ablation studies demonstrate that each design in PortaSpeech is effective.
Digital stethoscopes in combination with telehealth allow chest sounds to be easily collected and transmitted for remote monitoring and diagnosis. Chest sounds contain important information about a newborn's cardio-respiratory health. However, low-quality recordings complicate the remote monitoring and diagnosis. In this study, a new method is proposed to objectively and automatically assess heart and lung signal quality on a 5-level scale in real-time and to assess the effect of signal quality on vital sign estimation. For the evaluation, a total of 207 10s long chest sounds were taken from 119 preterm and full-term babies. Thirty of the recordings from ten subjects were obtained with synchronous vital signs from the Neonatal Intensive Care Unit (NICU) based on electrocardiogram recordings. As reference, seven annotators independently assessed the signal quality. For automatic quality classification, 400 features were extracted from the chest sounds. After feature selection using minimum redundancy and maximum relevancy algorithm, class balancing, and hyper-parameter optimization, a variety of multi-class and ordinal classification and regression algorithms were trained. Then, heart rate and breathing rate were automatically estimated from the chest sounds using adapted pre-existing methods. The results of subject-wise leave-one-out cross-validation show that the best-performing models had a mean squared error (MSE) of 0.49 and 0.61, and balanced accuracy of 57% and 51% for heart and lung qualities, respectively. The best-performing models for real-time analysis (<200ms) had MSE of 0.459 and 0.67, and balanced accuracy of 57% and 46%, respectively. Our experimental results underscore that increasing the signal quality leads to a reduction in vital sign error, with only high-quality recordings having a mean absolute error of less than 5 beats per minute, as required for clinical usage.
With the growing availability of smart devices and cloud services, personal speech assistance systems are increasingly used on a daily basis. Most devices redirect the voice recordings to a central server, which uses them for upgrading the recognizer model. This leads to major privacy concerns, since private data could be misused by the server or third parties. Federated learning is a decentralized optimization strategy that has been proposed to address such concerns. Utilizing this approach, private data is used for on-device training. Afterwards, updated model parameters are sent to the server to improve the global model, which is redistributed to the clients. In this work, we implement federated learning for speech recognition in a hybrid and an end-to-end model. We discuss the outcomes of these systems, which both show great similarities and only small improvements, pointing to a need for a deeper understanding of federated learning for speech recognition.
